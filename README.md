# Knowledge Distillation: Qwen2.5-32B-Instruct ‚Üí Qwen2.5-3B

‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏≠‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å **Qwen2.5-32B-Instruct** (Teacher) ‡∏™‡∏π‡πà **Qwen2.5-3B** (Student)

## Pipeline

```
Phase 1: SFT (Full Fine-Tuning)     ‚Üí  Phase 2: Logit Distillation (Full)
         Qwen2.5-3B base                       SFT checkpoint + Teacher 32B
         Opus Reasoning 3K                     MATH 12.5K
         ~30-45 min (H100)                     ~3-4 hrs (H100)
```

---

## üöÄ Quick Start

### 1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies

```bash
git clone https://github.com/Phonsiriwillbejommarn/Distillation.git
cd Distillation
pip install -r requirements.txt
```

### 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API Keys

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô **‡∏ó‡∏±‡πâ‡∏á 2 ‡πÑ‡∏ü‡∏•‡πå** (`sft_qwen.py` ‡πÅ‡∏•‡∏∞ `distill_qwen.py`):

```python
MY_WANDB_KEY = "‡πÉ‡∏™‡πà_wandb_key_‡∏à‡∏£‡∏¥‡∏á"
MY_HF_TOKEN = "‡πÉ‡∏™‡πà_hf_token_‡∏à‡∏£‡∏¥‡∏á"
```

### 3. Phase 1: SFT (Supervised Fine-Tuning)

‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏ó‡∏≥ reasoning ‡∏î‡πâ‡∏ß‡∏¢ dataset `nohurry/Opus-4.6-Reasoning-3000x-filtered`

```bash
python sft_qwen.py --config distill_config.yaml
```

‡πÇ‡∏°‡πÄ‡∏î‡∏• SFT ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ã‡∏ü‡∏ó‡∏µ‡πà `./sft_output/` ‡πÅ‡∏•‡∏∞ push ‡πÑ‡∏õ‡∏ó‡∏µ‡πà `Phonsiri/Qwen2.5-3B-SFT-Reasoning`

### 4. Phase 2: Knowledge Distillation

‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏≠‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å Teacher (32B) ‡∏™‡∏π‡πà Student (SFT checkpoint) ‡∏î‡πâ‡∏ß‡∏¢ dataset `rasbt/math_full_minus_math500`

```bash
python distill_qwen.py \
    --student_model ./sft_output \
    --config distill_config.yaml
```

‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ã‡∏ü‡∏ó‡∏µ‡πà `./distill_output/` ‡πÅ‡∏•‡∏∞ push ‡πÑ‡∏õ‡∏ó‡∏µ‡πà `Phonsiri/Qwen2.5-3B-Distilled`

---

## ‚è∏Ô∏è Resume ‡∏à‡∏≤‡∏Å Checkpoint

‡∏ñ‡πâ‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏±‡∏ô (GPU ‡∏´‡∏°‡∏î‡πÄ‡∏ß‡∏•‡∏≤, error, etc.):

```bash
# Auto-resume ‡∏à‡∏≤‡∏Å checkpoint ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
python distill_qwen.py \
    --student_model ./sft_output \
    --config distill_config.yaml \
    --resume_from_checkpoint auto

# ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏∞‡∏ö‡∏∏ checkpoint ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
python distill_qwen.py \
    --student_model ./sft_output \
    --config distill_config.yaml \
    --resume_from_checkpoint ./distill_output/checkpoint-500
```

> Checkpoint ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ã‡∏ü‡∏ó‡∏∏‡∏Å **100 steps** (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 5 ‡∏ï‡∏±‡∏ß)

---

## ‚öôÔ∏è Configuration

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà `distill_config.yaml`:

| Parameter | Default | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ |
|-----------|---------|---------|
| `teacher_model` | `Qwen/Qwen2.5-32B-Instruct` | ‡πÇ‡∏°‡πÄ‡∏î‡∏• Teacher |
| `student_model` | `Qwen/Qwen2.5-3B` | ‡πÇ‡∏°‡πÄ‡∏î‡∏• Student |
| `alpha` | `0.5` | ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å KL-div (0=SFT, 1=Distill ‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà) |
| `temperature` | `2.0` | ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ softmax (‡∏™‡∏π‡∏á=‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô) |
| `max_seq_length` | `8192` | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÇ‡∏ó‡πÄ‡∏Ñ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î |
| `per_device_train_batch_size` | `4` | Batch size ‡∏ï‡πà‡∏≠ GPU |
| `gradient_accumulation_steps` | `4` | Effective batch = 4√ó4 = 16 |
| `learning_rate` | `2e-5` | ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ |
| `num_train_epochs` | `3` | ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epochs |
| `save_steps` | `100` | ‡πÄ‡∏ã‡∏ü checkpoint ‡∏ó‡∏∏‡∏Å‡∏Å‡∏µ‡πà steps |
| `teacher_load_in_4bit` | `true` | ‡πÇ‡∏´‡∏•‡∏î teacher ‡πÅ‡∏ö‡∏ö 4-bit (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î VRAM) |

‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ override ‡∏ú‡πà‡∏≤‡∏ô CLI ‡πÑ‡∏î‡πâ:

```bash
python distill_qwen.py --alpha 0.7 --temperature 3.0 --learning_rate 1e-5
```

---

## üìä Loss Function

```
L = Œ± √ó KL(teacher_soft || student_soft) √ó T¬≤ + (1-Œ±) √ó CE(labels, student)
```

- **KL-divergence**: Student ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á teacher
- **Cross-Entropy**: Student ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å ground truth labels
- **T¬≤ scaling**: ‡∏ä‡∏î‡πÄ‡∏ä‡∏¢‡∏Å‡∏≤‡∏£ scale ‡∏Ç‡∏≠‡∏á gradients ‡∏à‡∏≤‡∏Å temperature

---

## üîç ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("Phonsiri/Qwen2.5-3B-Distilled")
tokenizer = AutoTokenizer.from_pretrained("Phonsiri/Qwen2.5-3B-Distilled")

messages = [{"role": "user", "content": "What is the sum of 1+2+3+...+100?"}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

inputs = tokenizer(text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=2048)
print(tokenizer.decode(outputs[0], skip_special_tokens=False))
```

Output ‡∏à‡∏∞‡∏°‡∏µ format:
```
<|im_start|>assistant
<think>
[‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î]
</think>

[‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢]
<|im_end|>
```

---

## üìÅ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå

```
.
‚îú‚îÄ‚îÄ distill_config.yaml       # Configuration ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
‚îú‚îÄ‚îÄ sft_qwen.py               # Phase 1: SFT (Full Fine-Tuning)
‚îú‚îÄ‚îÄ distill_qwen.py           # Phase 2: Logit Distillation (Full)
‚îú‚îÄ‚îÄ generate_teacher_data.py  # (Optional) ‡∏™‡∏£‡πâ‡∏≤‡∏á teacher responses
‚îú‚îÄ‚îÄ requirements.txt          # Dependencies
‚îî‚îÄ‚îÄ README.md                 # ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ
```

---

## üíª GPU Requirements

| GPU | SFT | Distillation | ‡∏£‡∏ß‡∏° |
|-----|-----|-------------|-----|
| H100 85GB | ~30 min | ~3-4 hrs | ~4-5 hrs |
| A100 80GB | ~1.5 hrs | ~8 hrs | ~10 hrs |
| A100 40GB | ~2 hrs | ~12 hrs | ~14 hrs |
| RTX 4090 24GB | ~3 hrs | ~18 hrs | ~21 hrs |

> VRAM ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥: ~38 GB (Teacher 32B 4-bit + Student 3B Full)

---

## üìà Multi-GPU

```bash
accelerate launch --num_processes 2 distill_qwen.py --config distill_config.yaml
```
