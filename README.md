# Knowledge Distillation: Qwen2.5-32B â†’ Qwen2.5-3B

à¸–à¹ˆà¸²à¸¢à¸—à¸­à¸”à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸ˆà¸²à¸ **Qwen2.5-32B-Instruct** (Teacher) à¸ªà¸¹à¹ˆ **Qwen2.5-3B** (Student) à¸œà¹ˆà¸²à¸™ 2 à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™

```
Phase 1: SFT (Full Fine-Tuning)          Phase 2: Logit Distillation
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qwen2.5-3B (Base)       â”‚          â”‚ Teacher: Qwen2.5-32B (4-bit)â”‚
â”‚ + Opus Reasoning 3K     â”‚  â”€â”€â”€â”€â”€â”€â–º â”‚ Student: SFT checkpoint      â”‚
â”‚ â†’ sft_output/           â”‚          â”‚ + MATH 12.5K                 â”‚
â”‚ ~30-45 min (H100)       â”‚          â”‚ â†’ distill_output/            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ ~3-4 hrs (H100)              â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ï¿½ à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡

```bash
git clone https://github.com/Phonsiriwillbejommarn/Distillation.git
cd Distillation
pip install -r requirements.txt
```

---

## ğŸ”‘ à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² API Keys

à¹à¸à¹‰à¹„à¸‚à¹ƒà¸™ **à¸—à¸±à¹‰à¸‡ `sft_qwen.py` à¹à¸¥à¸° `distill_qwen.py`**:

```python
MY_WANDB_KEY = "à¹ƒà¸ªà¹ˆ_wandb_key_à¸ˆà¸£à¸´à¸‡"   # https://wandb.ai/authorize
MY_HF_TOKEN  = "à¹ƒà¸ªà¹ˆ_hf_token_à¸ˆà¸£à¸´à¸‡"    # https://huggingface.co/settings/tokens (Write access)
```

---

## ğŸš€ Phase 1: SFT (Supervised Fine-Tuning)

à¸ªà¸­à¸™ Qwen2.5-3B base à¹ƒà¸«à¹‰à¸—à¸³ reasoning à¸”à¹‰à¸§à¸¢ [Opus Reasoning dataset](https://huggingface.co/datasets/nohurry/Opus-4.6-Reasoning-3000x-filtered)

```bash
python sft_qwen.py --config distill_config.yaml
```

| à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸” | à¸„à¹ˆà¸² |
|-----------|-----|
| à¹‚à¸¡à¹€à¸”à¸¥ | `Qwen/Qwen2.5-3B` (Base, Full Fine-Tuning) |
| Dataset | `nohurry/Opus-4.6-Reasoning-3000x-filtered` (3K à¸‚à¹‰à¸­) |
| Max tokens | 8192 |
| Batch size | 4 Ã— 4 = 16 (effective) |
| à¹€à¸§à¸¥à¸² (H100) | ~30-45 à¸™à¸²à¸—à¸µ |
| Output | `./sft_output/` + HF: `Phonsiri/Qwen2.5-3B-SFT-Reasoning` |

---

## ğŸ§  Phase 2: Knowledge Distillation

à¸–à¹ˆà¸²à¸¢à¸—à¸­à¸”à¸ˆà¸²à¸ Teacher 32B à¸ªà¸¹à¹ˆ Student (SFT checkpoint) à¸”à¹‰à¸§à¸¢ [MATH dataset](https://huggingface.co/datasets/rasbt/math_full_minus_math500)

```bash
python distill_qwen.py \
    --student_model ./sft_output \
    --config distill_config.yaml
```

| à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸” | à¸„à¹ˆà¸² |
|-----------|-----|
| Teacher | `Qwen/Qwen2.5-32B-Instruct` (4-bit quantized, frozen) |
| Student | `./sft_output` (Full Fine-Tuning) |
| Dataset | `rasbt/math_full_minus_math500` (12.5K à¸‚à¹‰à¸­) |
| Loss | `Î± Ã— KL(teacher âˆ¥ student) Ã— TÂ² + (1-Î±) Ã— CE` |
| Alpha | 0.5, Temperature: 2.0 |
| Checkpoint | à¹€à¸‹à¸Ÿà¸—à¸¸à¸ 100 steps â†’ push à¹„à¸› HF Hub |
| à¹€à¸§à¸¥à¸² (H100) | ~3-4 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡ |
| Output | `./distill_output/` + HF: `Phonsiri/Qwen2.5-3B-Math-Distilled` |

---

## â¸ï¸ Resume à¸ˆà¸²à¸ Checkpoint

à¸–à¹‰à¸² GPU à¸«à¸¥à¸¸à¸”à¸à¸¥à¸²à¸‡à¸„à¸±à¸™ à¸«à¸£à¸·à¸­à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸£à¸±à¸™à¸•à¹ˆà¸­à¸ˆà¸²à¸à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™:

**à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 1: à¸”à¸¶à¸‡à¸ˆà¸²à¸à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸£à¸±à¸™à¸¥à¹ˆà¸²à¸ªà¸¸à¸” (à¸‡à¹ˆà¸²à¸¢à¸—à¸µà¹ˆà¸ªà¸¸à¸”)**
```bash
python distill_qwen.py \
    --student_model ./sft_output \
    --config distill_config.yaml \
    --resume_from_checkpoint auto
```
à¸£à¸°à¸šà¸šà¸ˆà¸°à¹€à¸‚à¹‰à¸²à¹„à¸›à¸«à¸²à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¹ƒà¸™ `./distill_output` à¹à¸¥à¸°à¸—à¸³à¸•à¹ˆà¸­à¹ƒà¸«à¹‰à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´

**à¸§à¸´à¸˜à¸µà¸—à¸µà¹ˆ 2: à¸£à¸°à¸šà¸¸à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¹€à¸­à¸‡ (à¸à¸£à¸“à¸µà¹‚à¸«à¸¥à¸”à¸¡à¸²à¸ˆà¸²à¸ HuggingFace)**
à¸–à¹‰à¸²à¸¢à¹‰à¸²à¸¢à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡ à¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¹‚à¸«à¸¥à¸”à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ checkpoint à¸¡à¸²à¹„à¸§à¹‰à¹ƒà¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡ à¹à¸¥à¹‰à¸§à¸£à¸°à¸šà¸¸ path à¸•à¸£à¸‡à¹†:
```bash
python distill_qwen.py \
    --student_model ./sft_output \
    --config distill_config.yaml \
    --resume_from_checkpoint ./distill_output/last-checkpoint
```

ğŸš¨ *Checkpoint à¸—à¸¸à¸à¸­à¸±à¸™à¸ˆà¸°à¸—à¸¢à¸­à¸¢à¸–à¸¹à¸ Push à¸‚à¸¶à¹‰à¸™ Hugging Face Model Hub à¸‚à¸­à¸‡à¸„à¸¸à¸“à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¸«à¸²à¸à¸•à¸±à¹‰à¸‡ `push_to_hub: true`*

---

## ğŸ” à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹‚à¸¡à¹€à¸”à¸¥à¸«à¸¥à¸±à¸‡à¹€à¸—à¸£à¸™

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("Phonsiri/Qwen2.5-3B-Math-Distilled")
tokenizer = AutoTokenizer.from_pretrained("Phonsiri/Qwen2.5-3B-Math-Distilled")

messages = [{"role": "user", "content": "What is the sum of 1+2+3+...+100?"}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

inputs = tokenizer(text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=2048)
print(tokenizer.decode(outputs[0], skip_special_tokens=False))
```

à¹‚à¸¡à¹€à¸”à¸¥à¸ˆà¸°à¸•à¸­à¸šà¹ƒà¸™à¸£à¸¹à¸›à¹à¸šà¸š:
```
<|im_start|>assistant
<think>
[à¸à¸£à¸°à¸šà¸§à¸™à¸à¸²à¸£à¸„à¸´à¸” reasoning]
</think>

[à¸„à¸³à¸•à¸­à¸šà¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢]
<|im_end|>
```

---

## âš™ï¸ CLI Overrides

à¸—à¸¸à¸à¸„à¹ˆà¸²à¹ƒà¸™ `distill_config.yaml` à¸ªà¸²à¸¡à¸²à¸£à¸– override à¸œà¹ˆà¸²à¸™ CLI:

```bash
python distill_qwen.py --alpha 0.7 --temperature 3.0 --learning_rate 1e-5
python sft_qwen.py --max_seq_length 4096 --num_train_epochs 1
```

---

## ğŸ“ à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œ

```
â”œâ”€â”€ sft_qwen.py               # Phase 1: SFT
â”œâ”€â”€ distill_qwen.py           # Phase 2: Logit Distillation
â”œâ”€â”€ distill_config.yaml        # Configuration à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
â”œâ”€â”€ generate_teacher_data.py   # (Optional) à¸ªà¸£à¹‰à¸²à¸‡ teacher responses
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md
```

---

## ğŸ’» GPU Requirements

| GPU | SFT | Distillation | VRAM à¹ƒà¸Šà¹‰ |
|-----|-----|-------------|---------|
| **H100 85GB** | ~30 min | ~3-4 hrs | ~60 GB |
| A100 80GB | ~1.5 hrs | ~8 hrs | ~55 GB |
| A100 40GB | ~2 hrs | ~12 hrs | ~38 GB |

> VRAM à¸‚à¸±à¹‰à¸™à¸•à¹ˆà¸³: ~38 GB (Teacher 32B 4-bit + Student 3B Full)

---

## ğŸ“ˆ HuggingFace Hub Models

| Model | Repo |
|-------|------|
| SFT checkpoint | [Phonsiri/Qwen2.5-3B-Distilled](https://huggingface.co/Phonsiri/Qwen2.5-3B-Distilled) |
| Distilled (final) | [Phonsiri/Qwen2.5-3B-Math-Distilled](https://huggingface.co/Phonsiri/Qwen2.5-3B-Math-Distilled) |
